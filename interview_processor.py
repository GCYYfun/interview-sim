import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Optional, Tuple
import re
from datetime import datetime
import json
import traceback
import random
from menglong import Model
from menglong.ml_model.schema.ml_request import UserMessage as user

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams["font.sans-serif"] = ["SimHei", "Arial Unicode MS", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


class InterviewDataProcessor:
    """é¢è¯•æ•°æ®å¤„ç†å™¨ - ä¸“é—¨å¤„ç†interview_data.csv"""

    def __init__(self, csv_path: str = "interview_data.csv"):
        """
        åˆå§‹åŒ–é¢è¯•æ•°æ®å¤„ç†å™¨

        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
        """
        self.csv_path = Path(csv_path)
        self.df = None

        # Tokenç»Ÿè®¡ç›¸å…³
        self.token_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost": 0.0,
            "api_calls": 0,
        }

        # Checkpointç›¸å…³
        self.checkpoint_dir = Path("checkpoints")
        self.checkpoint_dir.mkdir(exist_ok=True)

        self.load_data()

    def load_data(self) -> pd.DataFrame:
        """åŠ è½½CSVæ•°æ®ï¼Œè‡ªåŠ¨å¤„ç†ç¼–ç é—®é¢˜"""
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
            encodings = ["utf-8", "gbk", "gb2312", "utf-8-sig"]

            for encoding in encodings:
                try:
                    self.df = pd.read_csv(self.csv_path, encoding=encoding)
                    print(f"âœ… æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æ•°æ®")
                    break
                except UnicodeDecodeError:
                    continue

            if self.df is None:
                raise ValueError("æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç æ–¹å¼è¯»å–æ–‡ä»¶")

            # æ•°æ®æ¸…ç†
            self._clean_data()

            print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {len(self.df)} è¡Œ Ã— {len(self.df.columns)} åˆ—")
            return self.df

        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            raise

    def _clean_data(self):
        """æ¸…ç†æ•°æ®"""
        try:
            # ç§»é™¤å®Œå…¨ç©ºçš„è¡Œå’Œåˆ—
            self.df = self.df.dropna(how="all").dropna(axis=1, how="all")

            # æ ‡å‡†åŒ–åˆ—å - ä½¿ç”¨å®é™…çš„åˆ—å
            print("åŸå§‹åˆ—å:", list(self.df.columns))

            # ä¿æŒåŸå§‹åˆ—åï¼ŒåªåšåŸºæœ¬æ¸…ç†
            self.df.columns = [col.strip() for col in self.df.columns]
        except Exception as e:
            print(f"âŒ æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            raise

    def _extract_response_text(self, response) -> str:
        """ä»ChatResponseå¯¹è±¡ä¸­æå–çº¯æ–‡æœ¬å†…å®¹"""
        try:
            # å°è¯•å¤šç§æ–¹å¼æå–textå†…å®¹
            if hasattr(response, "message") and hasattr(response.message, "content"):
                # å¤„ç† message.content ç»“æ„
                content = response.message.content
                if hasattr(content, "text"):
                    return content.text
                elif isinstance(content, str):
                    return content
                elif hasattr(content, "__str__"):
                    return str(content)

            # å°è¯•ç›´æ¥è·å–contentå±æ€§
            if hasattr(response, "content"):
                content = response.content
                if hasattr(content, "text"):
                    return content.text
                elif isinstance(content, str):
                    return content
                elif hasattr(content, "__str__"):
                    return str(content)

            # å°è¯•è·å–textå±æ€§
            if hasattr(response, "text"):
                return response.text

            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ä»å­—ç¬¦ä¸²è¡¨ç¤ºä¸­æå–
            response_str = str(response)

            # å°è¯•ä»å­—ç¬¦ä¸²ä¸­æå–textå†…å®¹
            import re

            text_match = re.search(r"text='([^']*)'", response_str)
            if text_match:
                return text_match.group(1)

            text_match = re.search(r'text="([^"]*)"', response_str)
            if text_match:
                return text_match.group(1)

            # å°è¯•æå–contentä¸­çš„text
            content_match = re.search(r"Content\(text='([^']*)'", response_str)
            if content_match:
                return content_match.group(1)

            content_match = re.search(r'Content\(text="([^"]*)"', response_str)
            if content_match:
                return content_match.group(1)

            # æœ€åçš„fallbackï¼Œè¿”å›å¤„ç†è¿‡çš„å­—ç¬¦ä¸²
            print("âš ï¸ æ— æ³•æå–textå†…å®¹ï¼Œä½¿ç”¨å­—ç¬¦ä¸²è¡¨ç¤º")
            return response_str

        except Exception as e:
            print(f"âŒ æå–å“åº”æ–‡æœ¬å¤±è´¥: {e}")
            return str(response)

    def _parse_indices_input(self, input_str: str, max_index: int) -> List[int]:
        """
        è§£æç”¨æˆ·è¾“å…¥çš„åºå·å­—ç¬¦ä¸²ï¼Œæ”¯æŒå•ä¸ªã€å¤šä¸ªã€èŒƒå›´

        Args:
            input_str: ç”¨æˆ·è¾“å…¥ï¼Œå¦‚ "2", "2,4,6", "2-5", "1,3-5,8"
            max_index: æœ€å¤§æœ‰æ•ˆç´¢å¼•

        Returns:
            è§£æåçš„ç´¢å¼•åˆ—è¡¨
        """
        if not input_str.strip():
            return []

        indices = []
        try:
            # åˆ†å‰²é€—å·åˆ†éš”çš„éƒ¨åˆ†
            parts = input_str.strip().split(",")

            for part in parts:
                part = part.strip()
                if not part:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯èŒƒå›´ï¼ˆåŒ…å«è¿å­—ç¬¦ï¼‰
                if "-" in part:
                    range_parts = part.split("-")
                    if len(range_parts) == 2:
                        start = int(range_parts[0].strip())
                        end = int(range_parts[1].strip())
                        # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
                        if (
                            start <= end
                            and 1 <= start <= max_index
                            and 1 <= end <= max_index
                        ):
                            indices.extend(range(start, end + 1))  # é—­åŒºé—´
                        else:
                            print(f"âš ï¸ æ— æ•ˆèŒƒå›´: {part} (æœ‰æ•ˆèŒƒå›´: 1-{max_index})")
                    else:
                        print(f"âš ï¸ æ— æ•ˆèŒƒå›´æ ¼å¼: {part}")
                else:
                    # å•ä¸ªæ•°å­—
                    index = int(part)
                    if 1 <= index <= max_index:
                        indices.append(index)
                    else:
                        print(f"âš ï¸ æ— æ•ˆç´¢å¼•: {index} (æœ‰æ•ˆèŒƒå›´: 1-{max_index})")

            # å»é‡å¹¶æ’åº
            return sorted(list(set(indices)))

        except ValueError as e:
            print(f"âŒ è§£æç´¢å¼•æ—¶å‡ºé”™: {e}")
            return []

    def display_data_preview(self, max_records: int = 10) -> int:
        """
        æ˜¾ç¤ºæ•°æ®é¢„è§ˆï¼Œä¾›ç”¨æˆ·é€‰æ‹©

        Args:
            max_records: æœ€å¤šæ˜¾ç¤ºçš„è®°å½•æ•°

        Returns:
            æœ‰æ•ˆæ•°æ®çš„æ€»æ•°é‡
        """
        if self.df is None or len(self.df) == 0:
            print("âŒ æ•°æ®ä¸ºç©º")
            return 0

        # ç­›é€‰æœ‰æ•ˆæ•°æ®
        required_columns = [
            "å€™é€‰äººè„±æ•ç®€å†",
            "å²—ä½è„±æ•jd",
            "ä¸€é¢é¢è¯•å¯¹è¯",
            "ä¸€é¢é¢è¯•è¯„ä»·",
        ]

        missing_columns = [
            col for col in required_columns if col not in self.df.columns
        ]
        if missing_columns:
            print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return 0

        valid_data = self.df.dropna(subset=required_columns)
        total_valid = len(valid_data)

        if total_valid == 0:
            print("âŒ æ²¡æœ‰åŒ…å«å®Œæ•´ä¿¡æ¯çš„æœ‰æ•ˆæ•°æ®")
            return 0

        print(f"\nğŸ“Š æ•°æ®é¢„è§ˆ (å…±{total_valid}æ¡æœ‰æ•ˆè®°å½•):")
        print("=" * 80)

        # æ˜¾ç¤ºé¢„è§ˆ
        display_count = min(max_records, total_valid)
        for i in range(display_count):
            row = valid_data.iloc[i]
            position = row.get("å²—ä½åç§°", "æœªçŸ¥å²—ä½")

            # è·å–ç®€å†æ‘˜è¦ï¼ˆå‰50å­—ç¬¦ï¼‰
            resume = str(row.get("å€™é€‰äººè„±æ•ç®€å†", ""))
            resume_preview = resume[:50] + "..." if len(resume) > 50 else resume

            # è·å–è¯„ä»·
            evaluation = str(row.get("ä¸€é¢é¢è¯•è¯„ä»·", "æ— è¯„ä»·"))
            evaluation_preview = (
                evaluation[:80] + "..." if len(evaluation) > 80 else evaluation
            )

            print(f"{i+1:2d}. å²—ä½: {position}")
            print(f"    ç®€å†: {resume_preview}")
            print(f"    è¯„ä»·: {evaluation_preview}")
            print()

        if total_valid > display_count:
            print(f"... è¿˜æœ‰ {total_valid - display_count} æ¡è®°å½•æœªæ˜¾ç¤º")

        print("=" * 80)
        print("ğŸ“ é€‰æ‹©è¯´æ˜:")
        print("  å•ä¸ªè®°å½•: 2")
        print("  å¤šä¸ªè®°å½•: 2,4,6,9")
        print("  è¿ç»­èŒƒå›´: 2-5 (åŒ…å«2,3,4,5)")
        print("  æ··åˆä½¿ç”¨: 1,3-5,8 (åŒ…å«1,3,4,5,8)")
        print()

        return total_valid

    # def _estimate_tokens(self, text: str) -> int:
    #     """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰"""
    #     # ç®€å•ä¼°ç®—ï¼šè‹±æ–‡çº¦4å­—ç¬¦=1tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦=1token
    #     chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
    #     english_chars = len(text) - chinese_chars
    #     estimated_tokens = int(chinese_chars / 1.5 + english_chars / 4)
    #     return estimated_tokens

    def _calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int,
        model_name: str = "us.anthropic.claude-sonnet-4-20250514-v1:0",
    ) -> float:
        """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
        # ä»·æ ¼è¡¨ï¼ˆæ¯1K tokensçš„ä»·æ ¼ï¼Œå•ä½ï¼šç¾å…ƒï¼‰
        pricing = {
            "us.anthropic.claude-sonnet-4-20250514-v1:0": {
                "input": 0.003,
                "output": 0.015,
            },
            "claude-sonnet-4-20250514": {
                "input": 0.003,
                "output": 0.015,
            },
        }

        if model_name not in pricing:
            model_name = (
                "us.anthropic.claude-sonnet-4-20250514-v1:0"  # é»˜è®¤ä½¿ç”¨claudeä»·æ ¼
            )

        input_cost = (input_tokens / 1000) * pricing[model_name]["input"]
        output_cost = (output_tokens / 1000) * pricing[model_name]["output"]

        return input_cost + output_cost

    def _update_token_stats_from_response(
        self, response, model_name: str = "us.anthropic.claude-sonnet-4-20250514-v1:0"
    ):
        """ä»chat responseä¸­æ›´æ–°tokenç»Ÿè®¡"""
        try:
            # å°è¯•ä»responseä¸­è·å–usageä¿¡æ¯
            usage = None
            if hasattr(response, "usage"):
                usage = response.usage
            elif (
                hasattr(response, "response_metadata")
                and "usage" in response.response_metadata
            ):
                usage = response.response_metadata["usage"]
            elif hasattr(response, "_raw_response") and hasattr(
                response._raw_response, "usage"
            ):
                usage = response._raw_response.usage

            if usage:
                # ä»usageä¸­è·å–tokenæ•°é‡
                input_tokens = getattr(usage, "input_tokens", 0) or getattr(
                    usage, "prompt_tokens", 0
                )
                output_tokens = getattr(usage, "output_tokens", 0) or getattr(
                    usage, "completion_tokens", 0
                )

                if input_tokens > 0 or output_tokens > 0:
                    cost = self._calculate_cost(input_tokens, output_tokens, model_name)

                    self.token_stats["total_input_tokens"] += input_tokens
                    self.token_stats["total_output_tokens"] += output_tokens
                    self.token_stats["total_cost"] += cost
                    self.token_stats["api_calls"] += 1

                    print(
                        f"ğŸ’° æœ¬æ¬¡è°ƒç”¨: è¾“å…¥{input_tokens}tokens, è¾“å‡º{output_tokens}tokens, æˆæœ¬${cost:.4f}"
                    )
                    return True

            # å¦‚æœæ— æ³•è·å–usageä¿¡æ¯ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•ä½œä¸ºfallback
            print("âš ï¸ æ— æ³•ä»responseä¸­è·å–usageä¿¡æ¯ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
            return False

        except Exception as e:
            print(f"âŒ æ›´æ–°tokenç»Ÿè®¡å¤±è´¥: {e}")
            return False

    # def _update_token_stats(
    #     self,
    #     input_text: str,
    #     output_text: str,
    #     model_name: str = "us.anthropic.claude-sonnet-4-20250514-v1:0",
    # ):
    #     """æ›´æ–°tokenç»Ÿè®¡ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
    #     # input_tokens = self._estimate_tokens(input_text)
    #     # output_tokens = self._estimate_tokens(output_text)
    #     cost = self._calculate_cost(input_tokens, output_tokens, model_name)

    #     self.token_stats["total_input_tokens"] += input_tokens
    #     self.token_stats["total_output_tokens"] += output_tokens
    #     self.token_stats["total_cost"] += cost
    #     self.token_stats["api_calls"] += 1

    #     print(
    #         f"ğŸ’° æœ¬æ¬¡è°ƒç”¨(ä¼°ç®—): è¾“å…¥{input_tokens}tokens, è¾“å‡º{output_tokens}tokens, æˆæœ¬${cost:.4f}"
    #     )

    def _save_single_experience(
        self, experience: Dict, record_id: int, sample_name: str
    ):
        """ä¿å­˜å•ä¸ªç»éªŒåˆ°ç‹¬ç«‹çš„JSONæ–‡ä»¶"""
        try:
            # æ–°å‘½åè§„èŒƒï¼šä¸ªäººé¢è¯•ç»éªŒæ€»ç»“
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ç²¾ç¡®åˆ°æ¯«ç§’
            filename = f"individual_interview_experience_{record_id}_{timestamp}.json"
            experience_file = self.checkpoint_dir / filename

            experience_data = {
                "record_id": record_id,
                "sample_name": sample_name,
                "timestamp": datetime.now().isoformat(),
                "experience": experience,
                "token_stats_snapshot": self.token_stats.copy(),
            }

            with open(experience_file, "w", encoding="utf-8") as f:
                json.dump(experience_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ ç»éªŒå·²ä¿å­˜: {experience_file.name}")
            return experience_file

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»éªŒå¤±è´¥: {e}")
            return None

    def _save_checkpoint(self, experiences: List[Dict], checkpoint_name: str):
        """ä¿å­˜checkpointï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        try:
            checkpoint_file = self.checkpoint_dir / f"{checkpoint_name}.json"

            checkpoint_data = {
                "timestamp": datetime.now().isoformat(),
                "experiences": experiences,
                "token_stats": self.token_stats.copy(),
                "total_processed": len(experiences),
            }

            with open(checkpoint_file, "w", encoding="utf-8") as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ Checkpointå·²ä¿å­˜: {checkpoint_file}")

        except Exception as e:
            print(f"âŒ ä¿å­˜checkpointå¤±è´¥: {e}")

    def list_experience_files(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å•ä¸ªç»éªŒæ–‡ä»¶"""
        try:
            experience_files = []
            pattern = "*_record_*.json"

            for file_path in self.checkpoint_dir.glob(pattern):
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)

                    experience_files.append(
                        {
                            "filename": file_path.name,
                            "record_id": data.get("record_id"),
                            "sample_name": data.get("sample_name"),
                            "timestamp": data.get("timestamp"),
                            "file_path": str(file_path),
                        }
                    )
                except Exception as e:
                    print(f"âš ï¸ è¯»å–ç»éªŒæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue

            # æŒ‰æ—¶é—´æˆ³æ’åº
            experience_files.sort(key=lambda x: x["timestamp"])
            return experience_files

        except Exception as e:
            print(f"âŒ åˆ—å‡ºç»éªŒæ–‡ä»¶å¤±è´¥: {e}")
            return []

    def load_all_experiences(self) -> List[Dict]:
        """åŠ è½½æ‰€æœ‰å•ä¸ªç»éªŒæ–‡ä»¶"""
        try:
            all_experiences = []
            experience_files = self.list_experience_files()

            for file_info in experience_files:
                try:
                    with open(file_info["file_path"], "r", encoding="utf-8") as f:
                        data = json.load(f)
                    all_experiences.append(data)
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ç»éªŒæ–‡ä»¶å¤±è´¥ {file_info['filename']}: {e}")
                    continue

            print(f"ğŸ“ å·²åŠ è½½ {len(all_experiences)} ä¸ªç»éªŒæ–‡ä»¶")
            return all_experiences

        except Exception as e:
            print(f"âŒ åŠ è½½æ‰€æœ‰ç»éªŒå¤±è´¥: {e}")
            return []

    def get_basic_info(self) -> Dict:
        """è·å–æ•°æ®åŸºæœ¬ä¿¡æ¯"""
        try:
            if self.df is None:
                return {}

            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy(obj):
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯numpyæ•°ç»„
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                # æ£€æŸ¥æ˜¯å¦æ˜¯pandasç±»å‹çš„ç¼ºå¤±å€¼
                elif isinstance(obj, (float, np.floating)):
                    try:
                        if pd.isna(obj) or np.isnan(obj):
                            return None
                    except (ValueError, TypeError):
                        pass
                    return float(obj)
                # æ£€æŸ¥å…¶ä»–ç±»å‹çš„ç¼ºå¤±å€¼
                elif obj is None:
                    return None
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, dict):
                    return {key: convert_numpy(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy(item) for item in obj]
                else:
                    # æœ€åå°è¯•æ£€æŸ¥pandasçš„ç¼ºå¤±å€¼
                    try:
                        if pd.isna(obj):
                            return None
                    except (ValueError, TypeError):
                        pass
                return obj

            info = {
                "total_records": len(self.df),
                "total_columns": len(self.df.columns),
                "columns": list(self.df.columns),
                "data_types": dict(self.df.dtypes.astype(str)),
                "missing_values": convert_numpy(dict(self.df.isnull().sum())),
                "sample_data": (
                    convert_numpy(self.df.head(2).to_dict("records"))
                    if len(self.df) > 0
                    else []
                ),
            }

            return info
        except Exception as e:
            print(f"âŒ è·å–åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            return {}

    def analyze_positions(self) -> pd.DataFrame:
        """åˆ†æå²—ä½åˆ†å¸ƒ"""
        try:
            # å¯»æ‰¾åŒ…å«å²—ä½ä¿¡æ¯çš„åˆ—
            position_cols = [
                col
                for col in self.df.columns
                if "å²—ä½" in col or "position" in col.lower()
            ]

            if not position_cols:
                print("âŒ æœªæ‰¾åˆ°å²—ä½ä¿¡æ¯åˆ—")
                return pd.DataFrame()

            position_col = position_cols[0]
            print(f"ğŸ“ ä½¿ç”¨åˆ— '{position_col}' è¿›è¡Œå²—ä½åˆ†æ")

            # åŸºæœ¬ç»Ÿè®¡
            position_stats = self.df[position_col].value_counts().to_frame("å€™é€‰äººæ•°é‡")

            # å¦‚æœæœ‰èªæ˜åº¦è¦æ±‚åˆ—
            intelligence_cols = [col for col in self.df.columns if "èªæ˜åº¦" in col]
            if intelligence_cols:
                intelligence_col = intelligence_cols[0]
                avg_intelligence = (
                    self.df.groupby(position_col)[intelligence_col]
                    .agg(["mean", "std"])
                    .round(2)
                )
                avg_intelligence.columns = ["å¹³å‡èªæ˜åº¦è¦æ±‚", "èªæ˜åº¦æ ‡å‡†å·®"]
                position_stats = position_stats.join(avg_intelligence)

            # å¦‚æœæœ‰é¢è¯•ç»“æœåˆ—
            result_cols = [
                col for col in self.df.columns if "é€šè¿‡" in col or "ç»“æœ" in col
            ]
            for result_col in result_cols[:2]:  # æœ€å¤šå¤„ç†2ä¸ªç»“æœåˆ—
                pass_count = self.df.groupby(position_col)[result_col].apply(
                    lambda x: x.astype(str).str.contains("é€šè¿‡", na=False).sum()
                )
                position_stats[f"{result_col}_é€šè¿‡äººæ•°"] = pass_count
                position_stats[f"{result_col}_é€šè¿‡ç‡"] = (
                    pass_count / position_stats["å€™é€‰äººæ•°é‡"] * 100
                ).round(1)

            return position_stats
        except Exception as e:
            print(f"âŒ å²—ä½åˆ†æå¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            return pd.DataFrame()

    def analyze_interview_results(self) -> Dict:
        """åˆ†æé¢è¯•ç»“æœ"""
        try:
            results = {}

            # å¯»æ‰¾é¢è¯•ç»“æœç›¸å…³åˆ—
            result_cols = [
                col for col in self.df.columns if "é€šè¿‡" in col or "ç»“æœ" in col
            ]

            for i, col in enumerate(result_cols[:2]):  # æœ€å¤šåˆ†æ2è½®é¢è¯•
                round_name = f"ç¬¬{i+1}è½®é¢è¯•" if i < 2 else col

                if col in self.df.columns:
                    col_results = self.df[col].astype(str).value_counts()
                    total = len(self.df[col].dropna())
                    pass_count = (
                        self.df[col].astype(str).str.contains("é€šè¿‡", na=False).sum()
                    )

                    results[round_name] = {
                        "ç»Ÿè®¡": {str(k): int(v) for k, v in dict(col_results).items()},
                        "é€šè¿‡ç‡": (
                            f"{(pass_count / total * 100):.1f}%" if total > 0 else "0%"
                        ),
                        "æ€»äººæ•°": int(total),
                    }

            return results
        except Exception as e:
            print(f"âŒ é¢è¯•ç»“æœåˆ†æå¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            return {}

    def search_candidates(self, keyword: str, column: str = None) -> pd.DataFrame:
        """æœç´¢å€™é€‰äºº"""
        if self.df is None:
            return pd.DataFrame()

        if column and column in self.df.columns:
            # åœ¨æŒ‡å®šåˆ—ä¸­æœç´¢
            mask = (
                self.df[column].astype(str).str.contains(keyword, case=False, na=False)
            )
        else:
            # åœ¨æ‰€æœ‰æ–‡æœ¬åˆ—ä¸­æœç´¢
            mask = pd.Series([False] * len(self.df))
            for col in self.df.select_dtypes(include=["object"]).columns:
                mask |= (
                    self.df[col].astype(str).str.contains(keyword, case=False, na=False)
                )

        return self.df[mask]

    def extract_candidate_info(self, row_index: int) -> Dict:
        """æå–å•ä¸ªå€™é€‰äººçš„è¯¦ç»†ä¿¡æ¯"""
        if row_index >= len(self.df):
            return {}

        candidate = self.df.iloc[row_index]

        # å¯»æ‰¾ç®€å†ç›¸å…³åˆ—
        resume_cols = [col for col in self.df.columns if "ç®€å†" in col]
        resume_info = {}

        if resume_cols and pd.notna(candidate[resume_cols[0]]):
            resume_text = str(candidate[resume_cols[0]])

            # æå–åŸºæœ¬ä¿¡æ¯
            patterns = {
                "æ±‚èŒæ„å‘": r"æ±‚èŒæ„å‘ï¼š(.+?)(?:\n|$)",
                "ç±è´¯": r"ç±è´¯ï¼š(.+?)(?:\n|$)",
                "å‡ºç”Ÿå¹´æœˆ": r"å‡ºç”Ÿå¹´æœˆï¼š(.+?)(?:\n|$)",
                "å­¦å†": r"å­¦å†ï¼š(.+?)(?:\n|$)",
                "æ”¿æ²»é¢è²Œ": r"æ”¿æ²»é¢è²Œï¼š(.+?)(?:\n|$)",
            }

            for key, pattern in patterns.items():
                match = re.search(pattern, resume_text)
                if match:
                    resume_info[key] = match.group(1).strip()

        return {
            "index": row_index,
            "resume_info": resume_info,
            "raw_data": candidate.to_dict(),
        }

    def plot_position_distribution(self, save_path: str = None):
        """ç»˜åˆ¶å²—ä½åˆ†å¸ƒå›¾"""
        position_cols = [col for col in self.df.columns if "å²—ä½" in col]

        if not position_cols:
            print("âŒ æœªæ‰¾åˆ°å²—ä½ä¿¡æ¯")
            return

        position_col = position_cols[0]

        plt.figure(figsize=(15, 5))

        # å²—ä½åˆ†å¸ƒé¥¼å›¾
        plt.subplot(1, 3, 1)
        position_counts = self.df[position_col].value_counts()
        plt.pie(position_counts.values, labels=position_counts.index, autopct="%1.1f%%")
        plt.title("å²—ä½åˆ†å¸ƒ")

        # å²—ä½æ•°é‡æŸ±çŠ¶å›¾
        plt.subplot(1, 3, 2)
        position_counts.plot(kind="bar")
        plt.title("å„å²—ä½å€™é€‰äººæ•°é‡")
        plt.ylabel("å€™é€‰äººæ•°")
        plt.xticks(rotation=45)

        # é€šè¿‡ç‡åˆ†æï¼ˆå¦‚æœæœ‰ç»“æœæ•°æ®ï¼‰
        result_cols = [col for col in self.df.columns if "é€šè¿‡" in col]
        if result_cols:
            plt.subplot(1, 3, 3)
            position_stats = self.analyze_positions()

            pass_rate_cols = [col for col in position_stats.columns if "é€šè¿‡ç‡" in col]
            if pass_rate_cols:
                position_stats[pass_rate_cols[0]].plot(kind="bar")
                plt.title(f"{pass_rate_cols[0]}")
                plt.ylabel("é€šè¿‡ç‡ (%)")
                plt.xticks(rotation=45)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.show()

    def extract_interview_experience(
        self,
        selected_indices: List[int] = None,
        resume_from_checkpoint: bool = True,
        summary_mode: str = "incremental",
    ) -> Dict:
        """
        ä»é¢è¯•æ•°æ®ä¸­æå–ç»éªŒæ€»ç»“

        Args:
            selected_indices: é€‰æ‹©çš„è®°å½•ç´¢å¼•åˆ—è¡¨ï¼ˆä»1å¼€å§‹ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™éšæœºé€‰æ‹©5æ¡
            resume_from_checkpoint: æ˜¯å¦ä»checkpointæ¢å¤ï¼Œé»˜è®¤True
            summary_mode: æ€»ç»“æ¨¡å¼
                - "incremental": å¢é‡æ›´æ–°ï¼ˆé»˜è®¤ï¼Œæ–°ç»éªŒä¸å·²æœ‰ç»éªŒåˆå¹¶ï¼‰
                - "review": æ¸©æ•…çŸ¥æ–°ï¼ˆåŒ…å«å†å²ç»éªŒçš„é‡æ–°æ€»ç»“ï¼‰
                - "full_refresh": å…¨é‡é‡æ–°æ€»ç»“ï¼ˆå®Œå…¨é‡æ–°åˆ†ææ‰€æœ‰ç»éªŒï¼‰

        Returns:
            åŒ…å«ç»éªŒæ€»ç»“çš„å­—å…¸
        """
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç´¢å¼•ï¼Œä½¿ç”¨é»˜è®¤éšæœºé€‰æ‹©5æ¡
            if selected_indices is None:
                selected_indices = []
                sample_size = 5
                print(
                    f"ğŸ” å¼€å§‹æå–é¢è¯•ç»éªŒï¼Œé»˜è®¤éšæœºé€‰æ‹© {sample_size} æ¡ï¼Œæ€»ç»“æ¨¡å¼: {summary_mode}"
                )
            else:
                sample_size = len(selected_indices)
                print(
                    f"ğŸ” å¼€å§‹æå–é¢è¯•ç»éªŒï¼Œé€‰æ‹©è®°å½•: {selected_indices}ï¼Œæ€»ç»“æ¨¡å¼: {summary_mode}"
                )

            # é‡ç½®tokenç»Ÿè®¡
            self.token_stats = {
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cost": 0.0,
                "api_calls": 0,
            }

            if self.df is None or len(self.df) == 0:
                return {"error": "æ•°æ®ä¸ºç©º"}

            # ç¡®ä¿éœ€è¦çš„åˆ—å­˜åœ¨
            required_columns = [
                "å€™é€‰äººè„±æ•ç®€å†",
                "å²—ä½è„±æ•jd",
                "ä¸€é¢é¢è¯•å¯¹è¯",
                "ä¸€é¢é¢è¯•è¯„ä»·",
            ]

            missing_columns = [
                col for col in required_columns if col not in self.df.columns
            ]
            if missing_columns:
                print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
                return {"error": f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}"}

            # ç­›é€‰æœ‰æ•ˆæ•°æ®ï¼ˆå››ä¸ªå­—æ®µéƒ½ä¸ä¸ºç©ºï¼‰
            valid_data = self.df.dropna(subset=required_columns)
            print(f"ğŸ“Š æœ‰æ•ˆæ•°æ®è¡Œæ•°: {len(valid_data)}/{len(self.df)}")

            if len(valid_data) == 0:
                return {"error": "æ²¡æœ‰åŒ…å«å®Œæ•´ä¿¡æ¯çš„æœ‰æ•ˆæ•°æ®"}

            # æ ¹æ®é€‰æ‹©çš„ç´¢å¼•è·å–æ•°æ®
            if selected_indices:
                # è½¬æ¢ä¸ºDataFrameç´¢å¼•ï¼ˆä»1å¼€å§‹è½¬ä¸ºä»0å¼€å§‹ï¼‰
                df_indices = [
                    i - 1 for i in selected_indices if 1 <= i <= len(valid_data)
                ]
                if not df_indices:
                    return {
                        "error": f"æ‰€é€‰æ‹©çš„ç´¢å¼•éƒ½æ— æ•ˆï¼Œæœ‰æ•ˆèŒƒå›´: 1-{len(valid_data)}"
                    }

                # æ ¹æ®ç´¢å¼•é€‰æ‹©æ•°æ®
                sampled_data = valid_data.iloc[df_indices]
                print(f"ğŸ“ å·²é€‰æ‹© {len(sampled_data)} æ¡è®°å½•: {selected_indices}")
            else:
                # é»˜è®¤éšæœºé‡‡æ ·
                if len(valid_data) > sample_size:
                    sampled_data = valid_data.sample(n=sample_size, random_state=42)
                    print(f"ğŸ“ å·²éšæœºé‡‡æ ·åˆ° {sample_size} æ¡è®°å½•")
                else:
                    sampled_data = valid_data
                    print(f"ğŸ“ ä½¿ç”¨å…¨éƒ¨ {len(sampled_data)} æ¡æœ‰æ•ˆè®°å½•")

            sample_name = f"experience_sample_{len(sampled_data)}"

            # å¦‚æœæ˜¯å…¨é‡é‡æ–°æ€»ç»“æ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†å²ç»éªŒ
            if summary_mode == "full_refresh":
                existing_experiences = self.load_all_experiences()
                if len(existing_experiences) < 2:
                    print("âš ï¸ å†å²ç»éªŒä¸è¶³ï¼Œæ”¹ä¸ºå¢é‡æ¨¡å¼")
                    summary_mode = "incremental"

            # å¤„ç†æ–°è®°å½•ï¼ˆæ¯æ¡ç»éªŒä¿å­˜ä¸ºå•ç‹¬æ–‡ä»¶ï¼‰
            new_experiences = []
            model = Model()

            for i, (idx, row) in enumerate(sampled_data.iterrows()):
                print(f"ğŸ¤– æ­£åœ¨åˆ†æç¬¬ {i + 1}/{len(sampled_data)} æ¡è®°å½•...")

                try:
                    # æ„é€ æç¤ºè¯
                    prompt = self._build_experience_extraction_prompt(
                        resume=row["å€™é€‰äººè„±æ•ç®€å†"],
                        jd=row["å²—ä½è„±æ•jd"],
                        conversation=row["ä¸€é¢é¢è¯•å¯¹è¯"],
                        evaluation=row["ä¸€é¢é¢è¯•è¯„ä»·"],
                    )

                    # è°ƒç”¨AIæ¨¡å‹
                    response = model.chat([user(content=prompt)])

                    # æå–å“åº”æ–‡æœ¬å†…å®¹ï¼ˆåªä¿å­˜textï¼Œä¸ä¿å­˜æ•´ä¸ªå¯¹è±¡ï¼‰
                    response_text = self._extract_response_text(response)

                    # æ›´æ–°tokenç»Ÿè®¡ - ä¼˜å…ˆä½¿ç”¨responseä¸­çš„usageä¿¡æ¯
                    if not self._update_token_stats_from_response(response):
                        # å¦‚æœæ— æ³•ä»responseè·å–usageï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•
                        # self._update_token_stats(prompt, response_text)
                        print("âš ï¸ æ— æ³•è·å–tokenä½¿ç”¨æƒ…å†µï¼Œè·³è¿‡ç»Ÿè®¡")

                    experience_data = {
                        "record_id": idx,
                        "resume_summary": str(row["å€™é€‰äººè„±æ•ç®€å†"])[:200] + "...",
                        "jd_summary": str(row["å²—ä½è„±æ•jd"])[:200] + "...",
                        "evaluation": str(row["ä¸€é¢é¢è¯•è¯„ä»·"]),
                        "extracted_experience": response_text,
                        "analysis_time": datetime.now().isoformat(),
                    }

                    # ä¿å­˜å•ä¸ªç»éªŒæ–‡ä»¶
                    experience_file = self._save_single_experience(
                        experience_data, idx, sample_name
                    )
                    if experience_file:
                        new_experiences.append(experience_data)

                except Exception as e:
                    print(f"âŒ åˆ†æè®°å½• {idx} æ—¶å‡ºé”™: {e}")
                    continue

            # æ˜¾ç¤ºæ€»tokenç»Ÿè®¡
            print(f"\nğŸ’° æ€»è®¡Tokenä½¿ç”¨ç»Ÿè®¡:")
            print(f"è¾“å…¥tokens: {self.token_stats['total_input_tokens']:,}")
            print(f"è¾“å‡ºtokens: {self.token_stats['total_output_tokens']:,}")
            print(f"APIè°ƒç”¨æ¬¡æ•°: {self.token_stats['api_calls']}")
            print(f"é¢„ä¼°æ€»æˆæœ¬: ${self.token_stats['total_cost']:.4f}")

            # æ ¹æ®æ¨¡å¼é€‰æ‹©è¦æ•´åˆçš„ç»éªŒ
            if summary_mode == "incremental":
                # å¢é‡æ¨¡å¼ï¼šåªæ•´åˆæ–°ç»éªŒ
                experiences_to_integrate = new_experiences
                print(f"ğŸ“Š å¢é‡æ¨¡å¼ï¼šæ•´åˆ {len(new_experiences)} æ¡æ–°ç»éªŒ")

            elif summary_mode == "review":
                # æ¸©æ•…çŸ¥æ–°æ¨¡å¼ï¼šæ–°ç»éªŒ + éƒ¨åˆ†å†å²ç»éªŒ
                all_experiences = self.load_all_experiences()
                # å–æœ€æ–°çš„å†å²ç»éªŒå’Œæ‰€æœ‰æ–°ç»éªŒ
                historical_experiences = [
                    exp["experience"] for exp in all_experiences[-5:]
                ]
                experiences_to_integrate = historical_experiences + new_experiences
                print(
                    f"ğŸ“Š æ¸©æ•…çŸ¥æ–°æ¨¡å¼ï¼šæ•´åˆ {len(historical_experiences)} æ¡å†å²ç»éªŒ + {len(new_experiences)} æ¡æ–°ç»éªŒ"
                )

            elif summary_mode == "full_refresh":
                # å…¨é‡é‡æ–°æ€»ç»“æ¨¡å¼ï¼šæ‰€æœ‰ç»éªŒ
                all_experiences = self.load_all_experiences()
                experiences_to_integrate = [
                    exp["experience"] for exp in all_experiences
                ] + new_experiences
                print(
                    f"ğŸ“Š å…¨é‡é‡æ–°æ€»ç»“æ¨¡å¼ï¼šæ•´åˆæ‰€æœ‰ {len(experiences_to_integrate)} æ¡ç»éªŒ"
                )

            else:
                print(f"âš ï¸ æœªçŸ¥æ€»ç»“æ¨¡å¼ {summary_mode}ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼")
                experiences_to_integrate = new_experiences

            # æ•´åˆç»éªŒ
            if experiences_to_integrate:
                print(f"ğŸ”„ æ­£åœ¨æ•´åˆ {len(experiences_to_integrate)} æ¡ç»éªŒ...")
                integrated_experience = self._integrate_experiences(
                    experiences_to_integrate, summary_mode
                )
            else:
                integrated_experience = "æš‚æ— ç»éªŒå¯æ•´åˆ"

            result = {
                "total_records": len(self.df),
                "valid_records": len(valid_data),
                "sampled_records": len(sampled_data),
                "successful_extractions": len(new_experiences),
                "new_experiences": new_experiences,
                "integrated_experience": integrated_experience,
                "summary_mode": summary_mode,
                "token_stats": self.token_stats.copy(),
                "extraction_time": datetime.now().isoformat(),
            }

            print(f"âœ… ç»éªŒæå–å®Œæˆ! æˆåŠŸæå– {len(new_experiences)} æ¡æ–°ç»éªŒ")
            return result

        except Exception as e:
            print(f"âŒ æå–é¢è¯•ç»éªŒå¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            return {"error": str(e), "traceback": traceback.format_exc()}

    def _build_experience_extraction_prompt(
        self, resume: str, jd: str, conversation: str, evaluation: str
    ) -> str:
        """æ„å»ºç»éªŒæå–çš„æç¤ºè¯"""
        return f"""
è¯·åŸºäºä»¥ä¸‹é¢è¯•æ•°æ®ï¼Œæå–HRåœ¨é¢è¯•ä¸­è¯†åˆ«å€™é€‰äºº"èªæ˜åº¦"ã€"çš®å®"å’Œ"å‹¤å¥‹"è¿™ä¸‰ä¸ªæŒ‡æ ‡çš„ç»éªŒæŠ€å·§ã€‚

## èƒŒæ™¯ä¿¡æ¯

### å€™é€‰äººç®€å†
{resume}

### å²—ä½JD
{jd}

### é¢è¯•å¯¹è¯
{conversation}

### HRè¯„ä»·
{evaluation}

## ä»»åŠ¡è¦æ±‚

è¯·åˆ†æè¿™æ¬¡é¢è¯•ä¸­HRæ˜¯å¦‚ä½•é€šè¿‡æé—®å’Œäº’åŠ¨æ¥è¯„ä¼°å€™é€‰äººçš„ï¼š
1. **èªæ˜åº¦** - é€»è¾‘æ€ç»´ã€å­¦ä¹ èƒ½åŠ›ã€é—®é¢˜åˆ†æèƒ½åŠ›
2. **çš®å®** - æŠ—å‹èƒ½åŠ›ã€éŸ§æ€§ã€é¢å¯¹å›°éš¾çš„æ€åº¦
3. **å‹¤å¥‹** - å·¥ä½œçƒ­æƒ…ã€ä¸»åŠ¨æ€§ã€æŒç»­å­¦ä¹ æ„æ„¿

è¯·ä»ä»¥ä¸‹è§’åº¦æå–ç»éªŒï¼š

### 1. æœ‰æ•ˆæé—®æŠ€å·§
- é’ˆå¯¹èªæ˜åº¦çš„æé—®æ–¹å¼å’Œè§’åº¦
- é’ˆå¯¹çš®å®çš„æé—®æ–¹å¼å’Œè§’åº¦  
- é’ˆå¯¹å‹¤å¥‹çš„æé—®æ–¹å¼å’Œè§’åº¦

### 2. å…³é”®è¿½é—®ç­–ç•¥
- å½“å€™é€‰äººå›ç­”ä¸å¤Ÿæ·±å…¥æ—¶çš„è¿½é—®æŠ€å·§
- å¦‚ä½•é€šè¿‡è¿½é—®æŒ–æ˜çœŸå®èƒ½åŠ›
- ä»€ä¹ˆæ ·çš„å›ç­”éœ€è¦è¿›ä¸€æ­¥éªŒè¯

### 3. è¯„ä¼°è¦ç‚¹
- æ¯ä¸ªæŒ‡æ ‡çš„å…³é”®è§‚å¯Ÿç‚¹
- ä¼˜ç§€å›ç­”çš„ç‰¹å¾
- éœ€è¦è­¦æƒ•çš„å›ç­”æ¨¡å¼

### 4. é€‚ç”¨åœºæ™¯
- è¿™äº›æŠ€å·§é€‚åˆä»€ä¹ˆç±»å‹çš„å²—ä½
- ä»€ä¹ˆèƒŒæ™¯çš„å€™é€‰äºº
- ä»€ä¹ˆé˜¶æ®µä½¿ç”¨

è¯·ç”¨ç»“æ„åŒ–çš„æ–¹å¼è¾“å‡ºï¼ŒåŒ…å«å…·ä½“çš„é—®é¢˜ç¤ºä¾‹å’Œåˆ¤æ–­æ ‡å‡†ã€‚
"""

    def _integrate_experiences(
        self, experiences: List[Dict], mode: str = "incremental"
    ) -> str:
        """
        æ•´åˆå¤šä¸ªç»éªŒæˆä¸ºç»¼åˆæŒ‡å¯¼

        Args:
            experiences: ç»éªŒåˆ—è¡¨
            mode: æ•´åˆæ¨¡å¼
                - "incremental": å¢é‡æ›´æ–°ï¼ˆé»˜è®¤ï¼Œæ–°ç»éªŒä¸å·²æœ‰ç»éªŒåˆå¹¶ï¼‰
                - "review": æ¸©æ•…çŸ¥æ–°ï¼ˆåŒ…å«å†å²ç»éªŒçš„é‡æ–°æ€»ç»“ï¼‰
                - "full_refresh": å…¨é‡é‡æ–°æ€»ç»“ï¼ˆå®Œå…¨é‡æ–°åˆ†ææ‰€æœ‰ç»éªŒï¼‰
        """
        try:
            if not experiences:
                return "æš‚æ— ç»éªŒå¯æ•´åˆ"

            print(f"ğŸ”„ æ­£åœ¨æ•´åˆ {len(experiences)} æ¡ç»éªŒï¼Œæ¨¡å¼: {mode}...")

            # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„æ•´åˆç­–ç•¥
            if mode == "incremental":
                prompt = self._build_incremental_prompt(experiences)
            elif mode == "review":
                prompt = self._build_review_prompt(experiences)
            elif mode == "full_refresh":
                prompt = self._build_full_refresh_prompt(experiences)
            else:
                print(f"âš ï¸ æœªçŸ¥æ¨¡å¼ {mode}ï¼Œä½¿ç”¨é»˜è®¤å¢é‡æ¨¡å¼")
                prompt = self._build_incremental_prompt(experiences)

            print(f"ğŸ“ æ•´åˆprompté•¿åº¦: {len(prompt)} å­—ç¬¦")

            model = Model()
            response = model.chat([user(content=prompt)])

            # ä»responseä¸­æå–çº¯æ–‡æœ¬å†…å®¹
            integrated_experience = self._extract_response_text(response)

            # æ›´æ–°tokenç»Ÿè®¡
            if not self._update_token_stats_from_response(response):
                # å¦‚æœæ— æ³•è·å–çœŸå®usageï¼Œä½¿ç”¨ä¼°ç®—
                # self._update_token_stats(prompt, integrated_experience)
                print("âš ï¸ æ— æ³•è·å–tokenä½¿ç”¨æƒ…å†µï¼Œè·³è¿‡ç»Ÿè®¡")

            return integrated_experience

        except Exception as e:
            print(f"âŒ æ•´åˆç»éªŒå¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            return "æ•´åˆç»éªŒæ—¶å‘ç”Ÿé”™è¯¯"

    def _build_incremental_prompt(self, experiences: List[Dict]) -> str:
        """æ„å»ºå¢é‡æ›´æ–°çš„prompt"""
        # é™åˆ¶ç»éªŒæ•°é‡å’Œå†…å®¹é•¿åº¦ä»¥é¿å…è¶…æ—¶
        max_experiences = 5
        max_chars_per_experience = 3000

        if len(experiences) > max_experiences:
            experiences = experiences[-max_experiences:]  # ä½¿ç”¨æœ€æ–°çš„ç»éªŒ

        experiences_text = ""
        for i, exp in enumerate(experiences, 1):
            exp_text = str(exp.get("extracted_experience", exp.get("experience", "")))
            if len(exp_text) > max_chars_per_experience:
                exp_text = exp_text[:max_chars_per_experience] + "..."
            experiences_text += f"\n## ç»éªŒ{i}:\n{exp_text}\n"

        return f"""è¯·å°†ä»¥ä¸‹é¢è¯•ç»éªŒè¿›è¡Œæ•´åˆï¼Œå½¢æˆä¸€ä»½ç®€æ´çš„HRé¢è¯•æŒ‡å¯¼æ‰‹å†Œã€‚

è¦æ±‚ï¼š
1. èšç„¦äº"èªæ˜åº¦"ã€"çš®å®"ã€"å‹¤å¥‹"ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦çš„è¯„ä¼°
2. æå–å…·ä½“å¯æ“ä½œçš„é¢è¯•æŠ€å·§å’Œé—®é¢˜
3. é¿å…é‡å¤ï¼Œçªå‡ºæ ¸å¿ƒè¦ç‚¹
4. è¾“å‡ºé•¿åº¦æ§åˆ¶åœ¨2000å­—ä»¥å†…

ç»éªŒå†…å®¹ï¼š
{experiences_text}

è¯·æ•´åˆæˆä¸€ä»½å®ç”¨çš„é¢è¯•æŒ‡å¯¼æ‰‹å†Œã€‚"""

    def _build_review_prompt(self, experiences: List[Dict]) -> str:
        """æ„å»ºæ¸©æ•…çŸ¥æ–°çš„prompt"""
        # é€‰æ‹©éƒ¨åˆ†å†å²ç»éªŒå’Œæ–°ç»éªŒ
        max_experiences = 8
        max_chars_per_experience = 2000

        if len(experiences) > max_experiences:
            # å–å‰é¢çš„å†å²ç»éªŒå’Œæœ€æ–°çš„ç»éªŒ
            historical = experiences[: max_experiences // 2]
            recent = experiences[-max_experiences // 2 :]
            experiences = historical + recent

        experiences_text = ""
        for i, exp in enumerate(experiences, 1):
            exp_text = str(exp.get("extracted_experience", exp.get("experience", "")))
            if len(exp_text) > max_chars_per_experience:
                exp_text = exp_text[:max_chars_per_experience] + "..."
            timestamp = exp.get("timestamp", exp.get("analysis_time", "æœªçŸ¥æ—¶é—´"))
            experiences_text += f"\n## ç»éªŒ{i} ({timestamp[:10] if timestamp else 'æœªçŸ¥æ—¶é—´'}):\n{exp_text}\n"

        return f"""è¯·å¯¹ä»¥ä¸‹é¢è¯•ç»éªŒè¿›è¡Œæ¸©æ•…çŸ¥æ–°å¼çš„æ•´åˆï¼Œæ—¢è¦ä¿ç•™ç»å…¸çš„é¢è¯•æŠ€å·§ï¼Œä¹Ÿè¦èå…¥æ–°çš„æ´å¯Ÿã€‚

è¦æ±‚ï¼š
1. é‡ç‚¹å…³æ³¨"èªæ˜åº¦"ã€"çš®å®"ã€"å‹¤å¥‹"çš„è¯„ä¼°æ–¹æ³•
2. æ€»ç»“å†å²ç»éªŒä¸­çš„æˆåŠŸæ¨¡å¼
3. è¯†åˆ«æ–°ç»éªŒä¸­çš„åˆ›æ–°ç‚¹
4. å½¢æˆæ—¢æœ‰ä¼ æ‰¿åˆæœ‰å‘å±•çš„æŒ‡å¯¼æ‰‹å†Œ
5. è¾“å‡ºé•¿åº¦æ§åˆ¶åœ¨2500å­—ä»¥å†…

ç»éªŒå†…å®¹ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰ï¼š
{experiences_text}

è¯·æ•´åˆæˆä¸€ä»½å…¼å…·ç»å…¸ä¸åˆ›æ–°çš„é¢è¯•æŒ‡å¯¼æ‰‹å†Œã€‚"""

    def _build_full_refresh_prompt(self, experiences: List[Dict]) -> str:
        """æ„å»ºå…¨é‡é‡æ–°æ€»ç»“çš„prompt"""
        # æ›´ä¸¥æ ¼çš„é™åˆ¶ï¼Œå› ä¸ºè¦å¤„ç†æ‰€æœ‰ç»éªŒ
        max_experiences = 10
        max_chars_per_experience = 1500

        if len(experiences) > max_experiences:
            # å‡åŒ€é‡‡æ ·
            step = len(experiences) // max_experiences
            experiences = experiences[::step][:max_experiences]

        experiences_text = ""
        for i, exp in enumerate(experiences, 1):
            exp_text = str(exp.get("extracted_experience", exp.get("experience", "")))
            if len(exp_text) > max_chars_per_experience:
                exp_text = exp_text[:max_chars_per_experience] + "..."
            experiences_text += f"\n## ç»éªŒ{i}:\n{exp_text}\n"

        return f"""è¯·å¯¹ä»¥ä¸‹æ‰€æœ‰é¢è¯•ç»éªŒè¿›è¡Œå…¨é¢é‡æ–°åˆ†æå’Œæ€»ç»“ï¼Œå½¢æˆä¸€ä»½å…¨æ–°çš„ç»¼åˆæ€§HRé¢è¯•æŒ‡å¯¼æ‰‹å†Œã€‚

è¦æ±‚ï¼š
1. ä»"èªæ˜åº¦"ã€"çš®å®"ã€"å‹¤å¥‹"ä¸‰ä¸ªç»´åº¦è¿›è¡Œç³»ç»Ÿæ€§åˆ†æ
2. æå–å…±æ€§è§„å¾‹å’Œå·®å¼‚åŒ–æ´å¯Ÿ
3. æ„å»ºå®Œæ•´çš„è¯„ä¼°æ¡†æ¶å’Œæ“ä½œæŒ‡å—
4. å»é™¤å†—ä½™ï¼Œçªå‡ºç²¾å
5. è¾“å‡ºé•¿åº¦æ§åˆ¶åœ¨3000å­—ä»¥å†…

æ‰€æœ‰ç»éªŒå†…å®¹ï¼š
{experiences_text}

è¯·é‡æ–°æ•´åˆæˆä¸€ä»½ç³»ç»Ÿæ€§çš„é¢è¯•æŒ‡å¯¼æ‰‹å†Œã€‚"""

    def list_checkpoints(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„checkpoint"""
        try:
            checkpoint_files = list(self.checkpoint_dir.glob("*.json"))
            checkpoint_names = [f.stem for f in checkpoint_files]

            if checkpoint_names:
                print("ğŸ“ å¯ç”¨çš„checkpoint:")
                for name in checkpoint_names:
                    checkpoint_data = self._load_checkpoint(name)
                    if checkpoint_data:
                        total_processed = checkpoint_data.get("total_processed", 0)
                        timestamp = checkpoint_data.get("timestamp", "æœªçŸ¥")
                        token_stats = checkpoint_data.get("token_stats", {})
                        cost = token_stats.get("total_cost", 0)
                        print(
                            f"  - {name}: {total_processed}æ¡è®°å½•, æˆæœ¬${cost:.4f}, æ—¶é—´:{timestamp}"
                        )
            else:
                print("ğŸ“ æ²¡æœ‰æ‰¾åˆ°checkpointæ–‡ä»¶")

            return checkpoint_names

        except Exception as e:
            print(f"âŒ åˆ—å‡ºcheckpointå¤±è´¥: {e}")
            return []

    def clear_checkpoints(self):
        """æ¸…ç©ºæ‰€æœ‰checkpoint"""
        try:
            checkpoint_files = list(self.checkpoint_dir.glob("*.json"))
            for file in checkpoint_files:
                file.unlink()
            print(f"ğŸ—‘ï¸ å·²æ¸…é™¤ {len(checkpoint_files)} ä¸ªcheckpointæ–‡ä»¶")
        except Exception as e:
            print(f"âŒ æ¸…é™¤checkpointå¤±è´¥: {e}")

    def save_experience_report(
        self, experience_data: Dict, output_path: str = None
    ) -> str:
        """ä¿å­˜ç»éªŒæŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            if output_path is None:
                # æ–°å‘½åè§„èŒƒï¼šé€šç”¨é¢è¯•æé—®ç»éªŒ
                version = "1.0"  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ç‰ˆæœ¬å·
                output_path = f"general_interview_guidelines_v{version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            # è½¬æ¢numpyç±»å‹
            def convert_numpy(obj):
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯numpyæ•°ç»„
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                # æ£€æŸ¥æ˜¯å¦æ˜¯pandasç±»å‹çš„ç¼ºå¤±å€¼
                elif isinstance(obj, (float, np.floating)):
                    try:
                        if pd.isna(obj) or np.isnan(obj):
                            return None
                    except (ValueError, TypeError):
                        pass
                    return float(obj)
                # æ£€æŸ¥å…¶ä»–ç±»å‹çš„ç¼ºå¤±å€¼
                elif obj is None:
                    return None
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, dict):
                    return {key: convert_numpy(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy(item) for item in obj]
                else:
                    # æœ€åå°è¯•æ£€æŸ¥pandasçš„ç¼ºå¤±å€¼
                    try:
                        if pd.isna(obj):
                            return None
                    except (ValueError, TypeError):
                        pass
                return obj

            clean_data = convert_numpy(experience_data)

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(clean_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ“‹ ç»éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            return output_path

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»éªŒæŠ¥å‘Šå¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            print(traceback.format_exc())
            return ""

    def export_analysis_report(self, output_path: str = None):
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        if output_path is None:
            output_path = f"interview_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_numpy(obj):
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯numpyæ•°ç»„
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            # æ£€æŸ¥æ˜¯å¦æ˜¯pandasç±»å‹çš„ç¼ºå¤±å€¼
            elif isinstance(obj, (float, np.floating)):
                try:
                    if pd.isna(obj) or np.isnan(obj):
                        return None
                except (ValueError, TypeError):
                    pass
                return float(obj)
            # æ£€æŸ¥å…¶ä»–ç±»å‹çš„ç¼ºå¤±å€¼
            elif obj is None:
                return None
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                # æœ€åå°è¯•æ£€æŸ¥pandasçš„ç¼ºå¤±å€¼
                try:
                    if pd.isna(obj):
                        return None
                except (ValueError, TypeError):
                    pass
            return obj

        basic_info = self.get_basic_info()
        position_analysis = (
            self.analyze_positions().to_dict("index")
            if not self.analyze_positions().empty
            else {}
        )
        interview_results = self.analyze_interview_results()

        report = {
            "analysis_time": datetime.now().isoformat(),
            "basic_info": convert_numpy(basic_info),
            "position_analysis": convert_numpy(position_analysis),
            "interview_results": convert_numpy(interview_results),
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return output_path

    def get_summary(self):
        """è·å–æ•°æ®æ‘˜è¦"""
        if self.df is None:
            return "æ•°æ®æœªåŠ è½½"

        position_cols = [col for col in self.df.columns if "å²—ä½" in col]
        position_info = ""
        if position_cols:
            position_counts = self.df[position_cols[0]].value_counts()
            position_info = f"\nä¸»è¦å²—ä½:\n{position_counts.head()}"

        summary = f"""
ğŸ“Š é¢è¯•æ•°æ®æ‘˜è¦
===============
æ€»è®°å½•æ•°: {len(self.df)}
æ•°æ®åˆ—æ•°: {len(self.df.columns)}
åˆ—å: {list(self.df.columns)}
{position_info}

é¢è¯•ç»“æœæ¦‚è§ˆ:
{self.analyze_interview_results()}
        """
        return summary


# ä½¿ç”¨ç¤ºä¾‹
def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        print("ğŸš€ å¼€å§‹å¤„ç†é¢è¯•æ•°æ®...")
        processor = InterviewDataProcessor("interview_data.csv")

        # æ‰“å°æ•°æ®æ‘˜è¦
        print(processor.get_summary())

        # åˆ†æå²—ä½
        print("\nğŸ¯ å²—ä½åˆ†æ:")
        position_analysis = processor.analyze_positions()
        if not position_analysis.empty:
            print(position_analysis)
        else:
            print("æ— å²—ä½åˆ†ææ•°æ®")

        # æœç´¢å€™é€‰äººç¤ºä¾‹
        print("\nğŸ” æœç´¢åŒ…å«'å¸‚åœº'çš„è®°å½•:")
        market_candidates = processor.search_candidates("å¸‚åœº")
        print(f"æ‰¾åˆ° {len(market_candidates)} ä¸ªç›¸å…³è®°å½•")

        # æŸ¥çœ‹ç¬¬ä¸€ä¸ªè®°å½•è¯¦æƒ…
        if len(processor.df) > 0:
            print("\nğŸ‘¤ ç¬¬ä¸€æ¡è®°å½•è¯¦æƒ…:")
            candidate_info = processor.extract_candidate_info(0)
            if candidate_info["resume_info"]:
                print(
                    "ç®€å†ä¿¡æ¯:",
                    json.dumps(
                        candidate_info["resume_info"], ensure_ascii=False, indent=2
                    ),
                )
            else:
                print("å‰å‡ åˆ—æ•°æ®:", dict(list(candidate_info["raw_data"].items())[:5]))

        # ç»˜åˆ¶å›¾è¡¨
        print("\nğŸ“Š ç”Ÿæˆæ•°æ®å¯è§†åŒ–...")
        processor.plot_position_distribution()

        # å¯¼å‡ºæŠ¥å‘Š
        print("\nğŸ“‹ å¯¼å‡ºåˆ†ææŠ¥å‘Š...")
        report_path = processor.export_analysis_report()

        print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        return processor

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        print(traceback.format_exc())
        return None


if __name__ == "__main__":
    processor = main()
